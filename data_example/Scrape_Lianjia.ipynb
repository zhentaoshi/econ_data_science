{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed2a69f",
   "metadata": {},
   "source": [
    "# Scraping Lianjia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0d1b5",
   "metadata": {},
   "source": [
    "We applied machine learning methods by training on a Beijing Lianjia transaction dataset to predict housing price in our paper \"*Unfolding Beijing in a Hedonic Way*\". See this paper here: https://www.researchgate.net/publication/355732617_Unfolding_Beijing_in_a_Hedonic_Way. \n",
    "\n",
    "Real transaction records used to be accessible on the Lianjia websites. But sadly, due to some policy restrictions arised in 2021 just after we finished the paper, Lianjia no longer shows the transaction webpages to the public. As a consequence, the webpage example we provided in the paper, https://bj.lianjia.com/chengjiao/101084782030.html, is no longer available. \n",
    "\n",
    "Here, for illustration purpose, we show how to scrape the on-sale second-hand property data  (https://bj.lianjia.com/ershoufang/), instead of the sold-out transaction data (https://bj.lianjia.com/chengjiao/) from the Beijing Lianjia website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea973fb",
   "metadata": {},
   "source": [
    "First, we need to import some packages, where: \n",
    "- `selenium` fetches the webpage source code； \n",
    "- `BeautifulSoup` organizes the fetched html code and searches the information we need in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb518734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4f71c",
   "metadata": {},
   "source": [
    "A webpage, e.g. **Firefox** or **Chrome**, is need for `selenium` to open a webpage in the background. To call the browser, we need a corresponding **webdriver**. To download and know more about the webdrivers, see: \n",
    "- Firefox: https://github.com/mozilla/geckodriver/releases; \n",
    "- Chrome: https://chromedriver.chromium.org/. \n",
    "\n",
    "Here, we use the Firefox webdriver as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8678ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.firefox.options.Options()\n",
    "options.headless = True # call the browser in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017277f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(options=options) # Firefox webdriver must be put into PATH (windows)\n",
    "driver.implicitly_wait(10) # set the maximum waiting time to load the webpage completely\n",
    "\n",
    "# If you want to use the Chrome webdriver, please uncomment and run the code in the following cell. \n",
    "\n",
    "# options = webdriver.chrome.options.Options()\n",
    "# options.headless = True\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "# driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fe58a",
   "metadata": {},
   "source": [
    "See an example objective webpage here: https://bj.lianjia.com/ershoufang/101114772718.html. What we want is a program that given a **url**, returns many information fields on the webpage, including attributes of the house, price, location, and etc. Here, we define a function `Scrape_Lianjia` to fulfill this demand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e91f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_Lianjia(url, driver):\n",
    "    \n",
    "    # Fetch the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Load the page into BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Initialize the container as a dictionary\n",
    "    info = {}\n",
    "    \n",
    "    # Basic information\n",
    "    labels = soup.find_all('span', class_='label')\n",
    "    for label in labels:\n",
    "        content = label.next_sibling\n",
    "        while content.text.strip()=='':\n",
    "            content = content.next_sibling\n",
    "        info[label.text.strip()] = content.text.strip().replace('\\xa0', ' ').replace('举报', '').replace('㎡', '平米')\n",
    "    info.pop('风险提示')\n",
    "    \n",
    "    # Follower\n",
    "    follow = soup.find('span', class_='count')\n",
    "    info['关注人数'] = follow.text.strip()\n",
    "    \n",
    "    # Total Price\n",
    "    price_total = soup.find('span', class_='total')\n",
    "    price_unit = soup.find('span', class_='unit')\n",
    "    info['总价'] = price_total.text.strip()+price_unit.text.strip()+'元'\n",
    "    \n",
    "    # Unit Price\n",
    "    unitPrice = soup.find('span', class_='unitPriceValue')\n",
    "    info['单价'] = unitPrice.text.strip()\n",
    "    \n",
    "    # Construction Year\n",
    "    constructYear = soup.find('div', class_='subInfo noHidden')\n",
    "    info['建成时间'] = re.findall(r'.+年建', constructYear.text.strip())[0]\n",
    "    \n",
    "    # Coordinates and Uniqueness\n",
    "    scripts = soup.find_all('script')\n",
    "    for script in scripts:\n",
    "        isUnique = re.findall(r\"isUnique:'\\w+'\", script.text.strip())\n",
    "        coord = re.findall(r\"resblockPosition:'\\d+\\.\\d+,\\d+\\.\\d+'\", script.text.strip())\n",
    "        if isUnique != []:\n",
    "            isUnique = re.split(\"[':]\", isUnique[0])\n",
    "            info['是否唯一'] = isUnique[2]\n",
    "        if coord != []:\n",
    "            coord = re.split(\"[':,]\", coord[0])\n",
    "            info['Longitude'] = coord[2]\n",
    "            info['Latitude'] = coord[3]\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abb1fc",
   "metadata": {},
   "source": [
    "See how this program work on the example webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624c0b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'小区名称': '莫奈花园',\n",
       " '所在区域': '顺义 后沙峪 五至六环',\n",
       " '看房时间': '提前预约随时可看',\n",
       " '链家编号': '101114772718',\n",
       " '房屋户型': '4室2厅1厨4卫',\n",
       " '所在楼层': '中楼层 (共4层)',\n",
       " '建筑面积': '212.87平米',\n",
       " '户型结构': '复式',\n",
       " '套内面积': '194.33平米',\n",
       " '建筑类型': '板楼',\n",
       " '房屋朝向': '南 北',\n",
       " '建筑结构': '混合结构',\n",
       " '装修情况': '精装',\n",
       " '梯户比例': '一梯九户',\n",
       " '供暖方式': '自供暖',\n",
       " '配备电梯': '无',\n",
       " '挂牌时间': '2022-04-05',\n",
       " '交易权属': '商品房',\n",
       " '上次交易': '2017-01-23',\n",
       " '房屋用途': '普通住宅',\n",
       " '房屋年限': '满五年',\n",
       " '产权所属': '非共有',\n",
       " '抵押信息': '有抵押 350万元 工商银行 客户偿还',\n",
       " '房本备件': '已上传房本照片',\n",
       " '关注人数': '20',\n",
       " '总价': '930万元',\n",
       " '单价': '43689元/平米',\n",
       " '建成时间': '2005年建',\n",
       " '是否唯一': '唯一住宅',\n",
       " 'Longitude': '116.53916',\n",
       " 'Latitude': '40.107599'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scrape_Lianjia('https://bj.lianjia.com/ershoufang/101114772718.html', driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b937a3b",
   "metadata": {},
   "source": [
    "The program works well. Since one url is corresponding for one house on sale, then the next question is how to get all these urls. It can be easily observed that the lists of the on-sale houses are shown on a series of webpages from https://bj.lianjia.com/ershoufang/pg1/ to https://bj.lianjia.com/ershoufang/pg100/ in a good order, where `pg` stands for \"page\" and there are 100 pages of house lists under https://bj.lianjia.com/ershoufang/. Here, as an example, we collect all housing urls in the first 5 pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93907068",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_max = 5 # set the number of pages to search here\n",
    "\n",
    "urls = []\n",
    "for p in list(range(1, page_max+1)):\n",
    "    driver.get('https://bj.lianjia.com/ershoufang/pg'+str(p)+'/')\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    items = soup.find_all('a', class_='noresultRecommend img LOGCLICKDATA')\n",
    "    for item in items:\n",
    "        urls.append(item['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7280f",
   "metadata": {},
   "source": [
    "See what are contained in the list `urls`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb5e209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://bj.lianjia.com/ershoufang/101115028158.html',\n",
       " 'https://bj.lianjia.com/ershoufang/101114580929.html',\n",
       " 'https://bj.lianjia.com/ershoufang/101115003779.html',\n",
       " 'https://bj.lianjia.com/ershoufang/101113640862.html',\n",
       " 'https://bj.lianjia.com/ershoufang/101111602994.html']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad709f",
   "metadata": {},
   "source": [
    "Finally, what we need to do is just run our program `Scrape_Lianjia` through all urls inside the list `urls`, and then save the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa662c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lianjia = pd.DataFrame()\n",
    "for url in urls:\n",
    "    info = Scrape_Lianjia(url, driver)\n",
    "    lianjia = lianjia.append(info, ignore_index=True)\n",
    "\n",
    "lianjia.to_csv('lianjia.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b1f54",
   "metadata": {},
   "source": [
    "See what we got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9547aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lianjia.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37765810",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning \n",
    "\n",
    "Zhentao Shi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning and artificial intelligence:\n",
    "\n",
    "* Technology or alchemy?\n",
    "* Statistics or biology?\n",
    "\n",
    "* [Tom Sargent](https://www.project-syndicate.org/commentary/artificial-intelligence-new-economic-models-by-thomas-j-sargent-2019-11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "* [ISLR] James, Gareth., Witten, Daniela., Hastie, Trevor., & Tibshirani, Robert. (2017). An introduction to statistical learning.  (Open access at https://www.statlearning.com/)\n",
    "* [ESL] Friendman, Hastie and Tibshirani (2001, 2008): Elements of Statistical Learning (Open access at https://hastie.su.domains/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Athey (2018) \n",
    "* Mullainathan and Spiess (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Connection between $X$ and $Y$\n",
    "* Regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A set of data fitting procedures focusing on out-of-sample prediction\n",
    "* Repeat a scientific experiment for $n$ times and obtain a dataset $(y_i, x_i)_{i=1}^n$.\n",
    "* How to best predict $y_{n+1}$ given $x_{n+1}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "* Only about $X$\n",
    "* Density estimation, principal component analysis, and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Conventional Statistics\n",
    "\n",
    "* Consistency\n",
    "* Asymptotic distribution (hopefully normal)\n",
    "* Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning's Responses\n",
    "\n",
    "* Efficiency is mostly irrelevant given big data\n",
    "* Statistical inference may not be the goal\n",
    "    * Recommendation system on Amazon or Taobao\n",
    "    * Care about the prediction accuracy, not the causal link\n",
    "* Is there a data generating process (DGP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonparametric Estimation\n",
    "\n",
    "* *Parametric*: a finite number of parameters\n",
    "* *Nonparametric*: an infinite number of parameters\n",
    "\n",
    "* Some ideas in nonparametric estimation is directly related to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Density Estimation\n",
    "\n",
    "* Density estimation given a sample $(x_1,\\ldots,x_n)$\n",
    "* If drawn from a parametric family, MLE for estimation\n",
    "* Misspecification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Histogram is nonparametric\n",
    "    * If grid too fine, small bias but large variance\n",
    "    * If grid too coarse, small variance but large bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 200\n",
    "\n",
    "par(mfrow = c(3, 3))\n",
    "par(mar = c(1, 1, 1, 1))\n",
    "\n",
    "x_base <- seq(0.01,1,by = 0.01)\n",
    "breaks_list = c(4, 12, 60)\n",
    "\n",
    "for (ii in 1:3){\n",
    "  x <- rbeta(n, 2, 2) # beta distribution\n",
    "  for ( bb in breaks_list){\n",
    "    hist(x, breaks = bb, main=\"\", freq = FALSE, ylim = c(0,3),xlim = c(0,1))\n",
    "    lines( y = dbeta( x_base, 2, 2), x = x_base , col = \"red\" )\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance-Bias Tradeoff\n",
    "\n",
    "![](graph/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Conditional Mean\n",
    "\n",
    "* Conditional mean $$f(x) = E[y_i |x_i = x]$$ given a sample $(y_i, x_i)$. \n",
    "* Solve \n",
    "$$\n",
    "\\min_f E[ (y_i - f(x_i) )^2 ]\n",
    "$$\n",
    "* In general $f(x)$ is a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Restrict the class of functions to search for minimizer\n",
    "    * Assume differentiability\n",
    "* One way is kernel method based on density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Series Estimation\n",
    "\n",
    "* Series expansion to approximate $f(x)$\n",
    "* Generates many additive regressors\n",
    "    * Ex: bounded, continuous and differentiate function has a series\n",
    "representation $f(x) = \\sum_{k=0}^{\\infty} \\beta_k \\cos (\\frac{k}{2}\\pi x )$.\n",
    "    * In finite sample, choose a finite $K$, usually much smaller than $n$\n",
    "    * Asymptotically $K \\to \\infty$ as $n \\to \\infty$ so that\n",
    "$$\n",
    "f_K(x) = \\sum_{k=0}^{K} \\beta_k \\cos \\left(\\frac{k}{2}\\pi x \\right) \\to f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Bias-variance trade-off\n",
    "    * Big $K$: small bias and large variance \n",
    "    * Small $K$: small variance and large bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Penalization\n",
    "\n",
    "* Specify a sufficiently large $K$, and then add a penalty term to control the complexity\n",
    "* Eg: *Ridge regression*: \n",
    "$$\n",
    "\\min_\\beta \\  \\frac{1}{2n}  \\sum_{i=1}^n \\left(y_i - \\sum_{k=0}^{K} \\beta_k f_k(x_i) \\right)^2\n",
    "+ \\lambda \\sum_{k=0}^K \\beta_k^2,\n",
    "$$\n",
    "where $\\lambda$ is the tuning parameter such that $\\lambda \\to 0$ as $n\\to \\infty$, and\n",
    "$f_k(x_i) = \\cos \\left(\\frac{k}{2}\\pi x_i \\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In compact notation, let $Y=(y_1,\\ldots,y_n)'$ and\n",
    "$X = (X_{ik} = f_k(x_i) )$, the above problem can be written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_2 ^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning Parameter\n",
    "\n",
    "* *Information criterion*: AIC, BIC\n",
    "* *Cross validation*\n",
    "\n",
    "\n",
    "* Active statistical research, but has little economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Econometrics Workflow\n",
    "\n",
    "![](graph/metric_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "![ ](graph/ML_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "\n",
    "* Machine learning's main purpose is often prediction\n",
    "* Agnostic about the DGP.\n",
    "* Models are measured by their performance in prediction.\n",
    "* Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Training dataset\n",
    "* Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Testing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Caret` Package\n",
    "\n",
    "* R package `caret` (Classification And REgression Training): a framework for many machine learning methods\n",
    "* The function [`createDataPartition`](https://topepo.github.io/caret/data-splitting.html)\n",
    "splits the sample for both cross sectional data and time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation (cross sectional data)\n",
    "\n",
    "* $S$-fold cross validation partitions the dataset into $S$ disjoint sections\n",
    "* Each iteration picks one of the sections as the (quasi) validation sample\n",
    "* The other $S-1$ sections as the training sample.\n",
    "* Compute an out-of-sample goodness-of-fit measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goodness of Fit (Out of Sample)\n",
    "\n",
    "* *Mean-squared prediction error* ${n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2$ where $val$ is the validation set and $n_v$ is its cardinality, \n",
    "* *Mean-absolute prediction error* ${n_v}^{-1}\\sum_{i \\in val} |y_i - \\hat{y}_i|$. \n",
    "* *Out of sample R-squared* (OOS $R^2$):\n",
    "\n",
    "$$\n",
    "1 - \\frac{{n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2}{{n_v}^{-1} \\sum_{i \\in val} y_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Repeat this process for $S$ times so that each of the $S$ sections are treated as the validation sample, \n",
    "* Average the goodness-of-fit measurement over the $S$ sections to determined the best tuning parameter. \n",
    "* In practice we can use  $S=5$ for 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Validation (time series data)\n",
    "\n",
    "* In time series context, cross validation must preserve the dependence structure. \n",
    "* If the time series is stationary, we can partition the data into $S$ consecutive blocks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(i will skip this slide)\n",
    "\n",
    "* If the purpose is forecasting, then we can use nested CV. \n",
    "![ ](graph/CV_Figure.png)\n",
    "\n",
    "* Nested CV with fixed-length rolling window scheme\n",
    "* The sub-training data can also be an extending rolling window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection\n",
    "\n",
    "* Number of covariates $x_i$ can be large.\n",
    "\n",
    "* Conventional attitude: prior knowledge\n",
    "* Recently economists wake up from the long lasting negligence.\n",
    "    * Stock and Watson (2012): forecasting 143 US macroeconomic indicators.\n",
    "    * A horse race of several variable selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lasso\n",
    "\n",
    "* least-absolute-shrinkage-and-selection-operator\n",
    "(Lasso) (Tibshirani, 1996)\n",
    "* Penalizes the $L_1$ norm of the coefficients.\n",
    "The criterion function of Lasso is written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_1\n",
    "$$\n",
    "where $\\lambda \\geq 0$ is a tuning parameter. \n",
    "\n",
    "Lasso shrinks some coefficients exactly to 0, in a wide range of values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![ ](graph/lasso_regression2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SCAD\n",
    "\n",
    "* Smoothly-clipped-absolute-deviation (SCAD) Fan and Li (2001):\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\sum_{j=1}^d \\rho_{\\lambda}( |\\beta_j| )\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\rho_{\\lambda}^{\\prime} (\\theta) = \\lambda \\left\\{ 1\\{\\theta\\leq \\lambda \\} +\n",
    "\\frac{(a\\lambda - \\theta)_+}{(a-1)\\lambda} \\cdot 1 \\{\\theta > \\lambda\\} \\right\\}\n",
    "$$\n",
    "for some $a>2$ and $\\theta>0$. \n",
    "\n",
    "* SCAD enjoys *oracle property*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaptive Lasso\n",
    "\n",
    "*Adaptive Lasso* (Zou, 2006) also enjoys the oracle property.\n",
    "\n",
    "Two-step algorithm:\n",
    "1. First run a Lasso or ridge regression and save the estimator $\\hat{\\beta}^{(1)}$\n",
    "2. Solve \n",
    "$(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\sum_{j=1}^d  w_j |\\beta_j|$\n",
    "where $w_j = 1 /  |\\hat{\\beta}_j^{(1)} |^a$ and $a\\geq 1$ is a constant. (Common choice is $a = 1$ or 2).\n",
    "\n",
    "* Lee, Shi and Gao (2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "R packages\n",
    "\n",
    "* `glmnet` or `LARS` implements Lasso\n",
    "* `ncvreg` carries out SCAD. \n",
    "* Adaptive Lasso by setting the weight via the argument `penalty.factor` in `glmnet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(magrittr)\n",
    "n <- 40\n",
    "p <- 50\n",
    "b0 <- c(rep(1, 10), rep(0, p - 10))\n",
    "x <- matrix(rnorm(n * p), n, p)\n",
    "y <- x %*% b0 + rnorm(n)\n",
    "\n",
    "ols <- MASS::ginv(t(x) %*% x) %*% (t(x) %*% y) # OLS\n",
    "# Implement Lasso by glmnet\n",
    "cv_lasso <- glmnet::cv.glmnet(x, y)\n",
    "lasso_result <- glmnet::glmnet(x, y, lambda = cv_lasso$lambda.min)\n",
    "\n",
    "# Get weights\n",
    "b_temp <- as.numeric(lasso_result$beta)\n",
    "b_temp[b_temp == 0] <- 1e-8\n",
    "w <- 1 / abs(b_temp) # Let gamma = 1\n",
    "\n",
    "# Implement Adaptive Lasso by glmnet\n",
    "cv_alasso <- glmnet::cv.glmnet(x, y, penalty.factor = w)\n",
    "alasso_result <-\n",
    "  glmnet::glmnet(x, y, penalty.factor = w, lambda = cv_alasso$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(b0, ylim = c(-0.8, 1.5), pch = 4, xlab = \"\", ylab = \"coefficient\")\n",
    "points(lasso_result$beta, col = \"red\", pch = 6)\n",
    "points(alasso_result$beta, col = \"blue\", pch = 5)\n",
    "points(ols, col = \"green\", pch = 3)\n",
    " \n",
    "# out of sample prediction\n",
    "x_new <- matrix(rnorm(n * p), n, p)\n",
    "y_new <- x_new %*% b0 + rnorm(n)\n",
    "lasso_msfe <- (y_new - predict(lasso_result, newx = x_new)) %>% var()\n",
    "alasso_msfe <- (y_new - predict(alasso_result, newx = x_new)) %>% var()\n",
    "ols_msfe <- (y_new - x_new %*% ols) %>% var()\n",
    "\n",
    "print(c(lasso_msfe, alasso_msfe, ols_msfe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DIY Lasso by `CVXR`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(CVXR)\n",
    "\n",
    "lambda <- 2 * cv_lasso$lambda.min # tuning parameter\n",
    "\n",
    "# CVXR for Lasso\n",
    "beta_cvxr <- Variable(p)\n",
    "obj <- sum_squares(y - x %*% beta_cvxr) / (2 * n) + lambda * p_norm(beta_cvxr, 1)\n",
    "prob <- Problem(Minimize(obj))\n",
    "lasso_cvxr <- solve(prob)\n",
    "beta_cvxr_hat <- lasso_cvxr$getValue(beta_cvxr) %>% as.vector() %>% print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stagewise Forward Selection\n",
    "\n",
    "More methods are available if prediction of the response variables is the sole purpose of the regression.\n",
    "\n",
    "Eg: *stagewise forward selection*\n",
    "\n",
    "1. Start from an empty model. \n",
    "2. Given many candidate $x_j$, in each round we add the regressor that can\n",
    "produce the biggest $R^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Close to the idea of *$L_2$ componentwise boosting*\n",
    "which does not adjust the coefficients fitted earlier\n",
    "\n",
    "* Shi and Huang (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Second Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction-Oriented Methods\n",
    "\n",
    "* Methods that induces data-driven interaction of the covariates.\n",
    "* Interaction makes the covariates much more flexible\n",
    "* Insufficient theoretical understanding\n",
    "* \"Black-boxes\" methods\n",
    "\n",
    "* Surprisingly superior performance\n",
    "* Industry insiders are pondering \"alchemy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Tree\n",
    "\n",
    "* Supervised learning: $x \\to y $\n",
    "* Regression tree (Breiman, 1984) recursively partitions the space of the regressors\n",
    "    * Each time a covariate is split into two dummies\n",
    "    * Splitting criterion is aggressive reduction of the SSR\n",
    "    * Tuning parameter is the depth of the tree\n",
    "    * Given a dataset $d$ and the depth of the tree, the fitted tree $\\hat{r}(d)$ is deterministic\n",
    "\n",
    "- Example: Using longitude and latitude for Beijing housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging\n",
    "\n",
    "* Tree is unstable\n",
    "* *Bootstrap averaging*, or *bagging*, reduces variance of trees (Breiman, 1996)\n",
    "    * Grow a tree for each bootstrap sample\n",
    "    * Simple average\n",
    "\n",
    "* An example of the *ensemble learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Inoue and Kilian (2008): an early application of bagging in time series forecast.\n",
    "* Hirano and Wright (2017): a theoretical perspective on the risk reduction of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "* *Random forest* (Breiman, 2001):\n",
    "    * Draw a bootstrap sample\n",
    "    * Before each split, shakes up the regressors by randomly sampling $m$ out of the total $p$ covarites. Stop until the depth of the tree is reached.\n",
    "    * Average the trees over the bootstrap samples\n",
    "    \n",
    "* The tuning parameters are the tree depth and $m$\n",
    "* More stable than bagging thanks to \"de-correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(randomForest)\n",
    "require(MASS)#Package which contains the Boston housing dataset\n",
    "attach(Boston)\n",
    "set.seed(101)\n",
    "\n",
    "#training Sample with 300 observations\n",
    "train=sample(1:nrow(Boston),300)\n",
    "\n",
    "Boston.rf=randomForest(medv ~ . , data = Boston, subset = train)\n",
    "plot(Boston.rf)\n",
    "\n",
    "# getTree(Boston.rf)\n",
    "importance(Boston.rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Consistency of random forest is not proved\n",
    "until Scornet, Biau, and Vert (2015)\n",
    "* Inferential theory was first established by\n",
    "Wager Athey (2018)  in the context of treatment effect estimation\n",
    "* Athey, Tibshirani, and Wager (2019) generalizes CART to local maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* Bagging and random forest use equal weight on each generated tree for the ensemble\n",
    "* Tree boosting takes a deterministic approach for the weights\n",
    "    1. Use the original data $d^0=(x_i,y_i)$ to grow a shallow tree $\\hat{r}^{0}(d^0)$. Save the prediction $f^0_i = \\alpha \\cdot \\hat{r}^0 (d^0, x_i)$ where\n",
    "   $\\alpha\\in [0,1]$ is a shrinkage tuning parameter. Save\n",
    "   the residual $e_i^{0} = y_i - f^0_i$. Set $m=1$.\n",
    "    2. In the $m$-th iteration, use the data $d^m = (x_i,e_i^{m-1})$ to grow a shallow tree $\\hat{r}^{m}(d^m)$. Save the prediction $f^m_i =  f^{m-1}_i +  \\alpha \\cdot \\hat{r}^m (d, x_i)$. Save\n",
    "   the residual $e_i^{m} = y_i - f^m_i$. Update $m = m+1$.\n",
    "    3. Repeat Step 2 until $m > M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Boosting has three tuning parameters: the tree depth,  the shrinkage level $\\alpha$, and the number of iterations $M$\n",
    "* The algorithm can be sensitive to any of the three tuning parameters\n",
    "* When a model is tuned well, it can performs remarkably\n",
    "    * Example: Beijing housing data.\n",
    "    * Gradient boosting via the package `gbm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Statisticians view boosting as a gradient descent algorithm to reduce the risk. The fitted\n",
    "tree in each iteration is the deepest descent direction, while the shrinkage tames the fitting to avoid proceeding too aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "\n",
    "\n",
    "load(\"data_example/lianjia.RData\")\n",
    "N <- nrow(lianjia) # a smaller sample\n",
    "lianjia <- lianjia[base::sample(1:N, round(N * 0.05 )), ]\n",
    "\n",
    "train_ind <- caret::createDataPartition(1:nrow(lianjia), p = 0.1)$Resample1\n",
    "# p = 0.1 to save time. Better to use p = 0.75\n",
    "\n",
    "gbmGrid <- expand.grid(\n",
    "  interaction.depth = seq(from = 10, to = 50, by = 30),\n",
    "  n.trees = seq(from = 1000, to = 10000, by = 4000),\n",
    "  shrinkage = c(0.01),\n",
    "  n.minobsinnode = 20\n",
    ")\n",
    "\n",
    "gbmControl <- caret::trainControl(method = \"cv\", number = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "formula.GBM <- price ~\n",
    "  square + livingRoom + drawingRoom + kitchen + bathRoom +\n",
    "  floor_type + floor_total + elevator + ladderRatio +\n",
    "  age + DOM + followers + fiveYearsProperty +\n",
    "  subway + district + Lng + Lat + t_trade +\n",
    "  communityAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(doParallel)\n",
    "library(gbm)\n",
    "\n",
    "gbmControl=trainControl(method=\"repeatedcv\",number=5,repeats=1)\n",
    "\n",
    "registerDoParallel(8)\n",
    "t=Sys.time()\n",
    "boostingReg=train(formula.GBM, \n",
    "                  data=lianjia[train_ind,],\n",
    "                  method=\"gbm\",\n",
    "                  distribution=\"gaussian\",\n",
    "                  trControl=gbmControl,\n",
    "                  tuneGrid=gbmGrid,\n",
    "                  metric=\"Rsquared\",\n",
    "                  verbose=F)\n",
    "cat(\"Time Cost of Finding Best Tuning Parameters:\",Sys.time()-t,\"\\n\")\n",
    "doParallel::stopImplicitCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "gbmTune = boostingReg$bestTune\n",
    "cat(\"The best tuning parameters for GBM are: \\n\");\n",
    "print(gbmTune)\n",
    "\n",
    "pred.boosting=predict(boostingReg,newdata=lianjia[-train_ind,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lmReg=lm(formula.GBM, data=lianjia[train_ind,])\n",
    "pred.lm=predict(lmReg,newdata=lianjia[-train_ind,])\n",
    "\n",
    "\n",
    "# Comparison\n",
    "\n",
    "target=lianjia[-train_ind,]$price\n",
    "cat(\"R-squared of GBM prediction =\",miscTools::rSquared(target,target-pred.boosting),\"\\n\")\n",
    "cat(\"R-squared of LM prediction =\",miscTools::rSquared(target,target-pred.lm),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Many variants of boosting algorithms\n",
    "    * $L_2$-boosting\n",
    "    * componentwise boosting\n",
    "    * AdaBoosting, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "* Artificial neural network (ANN) is the workhorse behind Alpha-Go and self-driven cars\n",
    "* A particular type of nonlinear models.\n",
    "\n",
    "![ANN](graph/Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "z_l^{(k)} & = & w_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = & g^{(k)} ( z_l^{(k)}), \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "where $ a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $g(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choice: sigmoid ($1/(1+\\exp(-x))$); ReLu, $z\\cdot 1\\{x\\geq 0\\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A user has several decisions to make\n",
    "* Activation function\n",
    "* Number of hidden layers\n",
    "* Number of nodes in each layer\n",
    "\n",
    "\n",
    "* Many free parameters are generated from the multiple layer and multiple nodes\n",
    "* In estimation often regularization methods are employed to penalize\n",
    "the $l_1$ and/or $l_2$ norms, which requires extra tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#  Theory is Underdeveloped\n",
    "\n",
    "* Theoretical understanding about its behavior is scant\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "    * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any\n",
    "measurable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computation\n",
    "\n",
    "* Free parameters must be determined by\n",
    "numerical optimization\n",
    "* Nonlinear complex structure makes the optimization\n",
    "very challenging and the global optimizer is beyond guarantee\n",
    "* De facto optimization algorithm\n",
    "is *stochastic gradient descent*\n",
    "\n",
    "* Google's `tensorflow`\n",
    "* `keras` is the deep learning modeling language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "* In optimization the update formula\n",
    "\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_{k} + a_k p_k,\n",
    "$$\n",
    "  \n",
    "  * step length $a_k \\in \\mathbb{R}$ \n",
    "  * vector direction $p_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Talyor expansion,\n",
    "$$\n",
    "f(\\beta_{k+1}) = f(\\beta_k + a_k p_k ) \\approx f(\\beta_k) + a_k \\nabla f(\\beta_k) p_k,\n",
    "$$\n",
    "\n",
    "* Choose $p_k$ to reduce $f(x)$ \n",
    "* A simple choice is $p_k =-\\nabla f(\\beta_k)$.\n",
    "\n",
    "c.f.:\n",
    "* Newton's method:$p_k =- (\\nabla^2 f(\\beta_k))^{-1}  \\nabla f(\\beta_k)$\n",
    "* BFGS uses a low-rank matrix to approximate $\\nabla^2 f(\\beta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* When sample size and/or number of parameter is big, prohibitively expensive to evaluate gradient\n",
    "* SGD uses a small batch of the sample to evaluate the gradient in each iteration. \n",
    "\n",
    "* SGD involves tuning parameters \n",
    "  * say, batch size\n",
    "  * learning rate\n",
    "\n",
    "* Careful experiments must be carried out before serious implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Experiment\n",
    "\n",
    "Use SGD in the PPMLE\n",
    "* sample size 100,000\n",
    "* the number of parameters 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "poisson.loglik = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  exp( X %*% b )\n",
    "  ell = -mean( -lambda + y *  log(lambda) )\n",
    "  return(ell)\n",
    "}\n",
    "\n",
    "\n",
    "poisson.loglik.grad = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  as.vector( exp( X %*% b ) )\n",
    "  ell = -colMeans( -lambda * X + y * X )\n",
    "  ell_eta = ell\n",
    "  return(ell_eta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##### generate the artificial data\n",
    "set.seed(898)\n",
    "nn = 1e5; K = 100\n",
    "\n",
    "X = cbind(1, matrix( runif( nn*(K-1) ), ncol = K-1 ) )\n",
    "b0 = rep(1, K) / K\n",
    "y = rpois(nn, exp( X %*% b0 ) )\n",
    "\n",
    "\n",
    "b.init = runif(K); b.init  = 2 * b.init / sum(b.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# and these tuning parameters are related to N and K\n",
    "\n",
    "n = length(y)\n",
    "test_ind = sample(1:n, round(0.2*n) ) \n",
    "\n",
    "# 80% training data\n",
    "# 20% testing data\n",
    "\n",
    "y_test = y[test_ind]\n",
    "X_test = X[test_ind, ]\n",
    "\n",
    "y_train = y[-test_ind ]\n",
    "X_train = X[-test_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "\n",
    "# sgd depends on\n",
    "# * eta: the learning rate\n",
    "# * epoch: the averaging small batch\n",
    "# * the initial value\n",
    "\n",
    "set.seed(105)\n",
    "\n",
    "max_iter = 5000\n",
    "min_iter = 20\n",
    "eta=0.01\n",
    "epoch = round( 100*sqrt(K) )\n",
    "\n",
    "\n",
    "b_old = b.init\n",
    "\n",
    "pts0 = Sys.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# the iteration of gradient\n",
    "for (i in 1:max_iter ){\n",
    "\n",
    "  loglik_old = poisson.loglik(b_old, y_train, X_train)\n",
    "  i_sample = sample(1:length(y_train), epoch, replace = TRUE )\n",
    "  b_new = b_old - eta * poisson.loglik.grad(b_old, y_train[i_sample], X_train[i_sample, ])\n",
    "  loglik_new = poisson.loglik(b_new, y_test, X_test)\n",
    "  b_old = b_new # update\n",
    "\n",
    "  criterion =  loglik_old - loglik_new  \n",
    "  if (  criterion < 0.0001 & i >= min_iter ) break\n",
    "}\n",
    "cat(\"point estimate =\", b_new, \", log_lik = \", loglik_new, \"\\n\")\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# optimx is too slow for this dataset.\n",
    "# Nelder-Mead method is too slow for this dataset\n",
    "\n",
    "# thus we only sgd with NLoptr\n",
    "\n",
    "opts = list(\"algorithm\"=\"NLOPT_LD_SLSQP\",\"xtol_rel\"=1.0e-7, maxeval = 5000)\n",
    "\n",
    "\n",
    "pts0 = Sys.time( )\n",
    "res_BFGS = nloptr::nloptr( x0=b.init,\n",
    "                 eval_f=poisson.loglik,\n",
    "                 eval_grad_f = poisson.loglik.grad,\n",
    "                 opts=opts,\n",
    "                 y = y_train, X = X_train)\n",
    "print( res_BFGS )\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)\n",
    "\n",
    "b_hat_nlopt = res_BFGS$solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#### evaluation in the test sample\n",
    "cat(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
    "cat(\"log lik in test data by sgd = \", poisson.loglik(b_new, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by nlopt = \", poisson.loglik(b_hat_nlopt, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by oracle = \", poisson.loglik(b0, y = y_test, X_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Mature algorithms for implementation\n",
    "* Theoretical investigation is in progress\n",
    "* Economic applications are emerging"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
